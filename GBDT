# GBDT算法梳理

## 前向分佈算法

前向分佈算法也是基於之前的分類器的剩餘損失做優化，它每一輪都選取最小化原分類器的殘差來定義新學習器的參數。

## 負梯度擬合

gradient boosting即根據梯度最陡的方向去降低損失函數。

## 損失函數

### 分類模型

1. deviance
2. exponential
3. 對數損失函數

### 回歸模型

1. 均方差ls
2. 絕對損失lad
3. huber損失huber
4. 分位數損失quantile

## 正則化

目的是防止過擬合。
1. 步長(learning rate)(0,1]

將上一個學習器的結果乘上步長。

2. 子采樣比例(subsample)(0,1]
3. 正則化剪枝

## 優缺點

\+ 可以處理連續值和離散值。

\+ 準確率較高。

\+ 對異常值的魯棒性強（相較於adaboost），但要選擇魯棒性強的損失函數。

\- 不能并行訓練數據。

## sklearn參數

參見https://www.cnblogs.com/pinard/p/6143927.html

## 應用場景

搜索排序
點擊率預估

## 參考來源

1. https://blog.csdn.net/bitcarmanlee/article/details/77918131
2. 正則化 https://zhuanlan.zhihu.com/p/29802325
