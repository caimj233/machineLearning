# 随机森林算法梳理
## 集成学习概念
集成学习方法使用**多种学习算法**来获得比单独使用任何单独的学习算法更好的预测性能。

需做兩項決策：
1. 个体学习器的選擇
2. 結合策略的選擇

## 个体学习器概念
基於某個機器學習的算法實現的對數據的學習器，多個個體學習器組成一個強學習器。

一組個體學習器可按性質分兩種，同質和異質。同質是指該組學習器中的個體學習器都是同一個種類的，如都是決策樹。

## boosting bagging
是兩種結合策略。
### boosting
有點像貪心算法，在當前所有學習器效果最優的情況下，再叠加下一個學習器的預測結果。

比如AdaBoost是投票法，但每個個體學習器的權重不同。增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。

### bagging
对分类问题：将個體學習器采用投票的方式得到分类结果；对回归问题，计算個體學習器的均值作为最后的结果

## 结合策略(平均法，投票法，学习法)
### 平均法：简单平均、加权平均 
把結果進行平均。

### 投票法： 
1.绝对多数投票法：某标记超过半数； 
2.相对多数投票法：预测为得票最多的标记，若同时有多个标记的票最高，则从中随机选取一个。 
3.加权投票法：提供了预测结果，与加权平均法类似。

### 学习法 
Stacking描述：先从初始数据集中训练出初级学习器，然后“生成”一个新数据集用于训练次级学习器。在新数据集中，初级学习器的输出被当做样例输入特征，初始样本的标记仍被当做样例标记。

## 随机森林思想
随机森林将决策树用作bagging中的模型。首先，用bootstrap方法生成m个训练集，然后，对于每个训练集，构造一颗决策树，在节点找特征进行分裂的时候，并不是对所有特征找到能使得指标（如信息增益）最大的，而是在特征中随机抽取一部分特征，在抽到的特征中间找到最优解，应用于节点，进行分裂。

随机森林的方法由于有了bagging，也就是集成的思想在，实际上相当于对于样本和特征都进行了采样（如果把训练数据看成矩阵，就像实际中常见的那样，那么就是一个行和列都进行采样的过程），所以可以避免过拟合。

## 优缺点
\+ 可以計算各實例的親近度，對於數據挖掘、檢測離群點和數據可視化非常有用。 
\+ 它能够处理很高维度（feature很多）的数据，并且不用做特征选择，对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据集无需规范化。
\- 在某些噪聲較大的分類和回歸問題上會過擬合。 
對於有不同級別的屬性的數據，級別劃分較多的屬性會對隨機森林產生更大的影響，所以隨機森林在這種數據上產生的屬性權值是不可信的。

## sklearn参数
詳見 https://blog.csdn.net/jeryjeryjery/article/details/78882661
